{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b012d768",
   "metadata": {},
   "source": [
    "## Ensemble Learning (Voting classifier )\n",
    "### A presentation by Mohammad Kahkeshani Python | AI teacher\n",
    "### You can access the relevant files through my GitHub at the following address:\n",
    "#### https://github.com/mkahkeshani/ML_Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45b50c",
   "metadata": {},
   "source": [
    "#### lets import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf62234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d216c695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return x_y can split our data\n",
    "# as frame = show data like pandas\n",
    "x , y = load_breast_cancer(return_X_y= True ,as_frame= True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d71903",
   "metadata": {},
   "source": [
    "### in this Dataset our target is: diagnosis\n",
    "### and our featurs are: radius_mean میانگین شعاع\n",
    "### texture_mean میانگین بافت\n",
    "### in target column we can see M and B:\n",
    "### M = malignant بد خیم\n",
    "### B = benign "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcce3ae",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08dd9eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>10.57</td>\n",
       "      <td>20.22</td>\n",
       "      <td>70.15</td>\n",
       "      <td>338.3</td>\n",
       "      <td>0.09073</td>\n",
       "      <td>0.16600</td>\n",
       "      <td>0.22800</td>\n",
       "      <td>0.05941</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>0.08450</td>\n",
       "      <td>...</td>\n",
       "      <td>10.85</td>\n",
       "      <td>22.82</td>\n",
       "      <td>76.51</td>\n",
       "      <td>351.9</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.3619</td>\n",
       "      <td>0.60300</td>\n",
       "      <td>0.14650</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>13.46</td>\n",
       "      <td>28.21</td>\n",
       "      <td>85.89</td>\n",
       "      <td>562.1</td>\n",
       "      <td>0.07517</td>\n",
       "      <td>0.04726</td>\n",
       "      <td>0.01271</td>\n",
       "      <td>0.01117</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>0.05763</td>\n",
       "      <td>...</td>\n",
       "      <td>14.69</td>\n",
       "      <td>35.63</td>\n",
       "      <td>97.11</td>\n",
       "      <td>680.6</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.07934</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.07061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>13.66</td>\n",
       "      <td>15.15</td>\n",
       "      <td>88.27</td>\n",
       "      <td>580.6</td>\n",
       "      <td>0.08268</td>\n",
       "      <td>0.07548</td>\n",
       "      <td>0.04249</td>\n",
       "      <td>0.02471</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>0.05897</td>\n",
       "      <td>...</td>\n",
       "      <td>14.54</td>\n",
       "      <td>19.64</td>\n",
       "      <td>97.96</td>\n",
       "      <td>657.0</td>\n",
       "      <td>0.1275</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.25690</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.3387</td>\n",
       "      <td>0.09638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>11.08</td>\n",
       "      <td>18.83</td>\n",
       "      <td>73.30</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.21540</td>\n",
       "      <td>0.16890</td>\n",
       "      <td>0.06367</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07950</td>\n",
       "      <td>...</td>\n",
       "      <td>13.24</td>\n",
       "      <td>32.82</td>\n",
       "      <td>91.76</td>\n",
       "      <td>508.1</td>\n",
       "      <td>0.2184</td>\n",
       "      <td>0.9379</td>\n",
       "      <td>0.84020</td>\n",
       "      <td>0.25240</td>\n",
       "      <td>0.4154</td>\n",
       "      <td>0.14030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>11.27</td>\n",
       "      <td>12.96</td>\n",
       "      <td>73.16</td>\n",
       "      <td>386.3</td>\n",
       "      <td>0.12370</td>\n",
       "      <td>0.11110</td>\n",
       "      <td>0.07900</td>\n",
       "      <td>0.05550</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.06914</td>\n",
       "      <td>...</td>\n",
       "      <td>12.84</td>\n",
       "      <td>20.53</td>\n",
       "      <td>84.93</td>\n",
       "      <td>476.1</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.22470</td>\n",
       "      <td>0.13180</td>\n",
       "      <td>0.3343</td>\n",
       "      <td>0.09215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "376        10.57         20.22           70.15      338.3          0.09073   \n",
       "377        13.46         28.21           85.89      562.1          0.07517   \n",
       "378        13.66         15.15           88.27      580.6          0.08268   \n",
       "379        11.08         18.83           73.30      361.6          0.12160   \n",
       "380        11.27         12.96           73.16      386.3          0.12370   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "376           0.16600         0.22800              0.05941         0.2188   \n",
       "377           0.04726         0.01271              0.01117         0.1421   \n",
       "378           0.07548         0.04249              0.02471         0.1792   \n",
       "379           0.21540         0.16890              0.06367         0.2196   \n",
       "380           0.11110         0.07900              0.05550         0.2018   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...         25.38          17.33   \n",
       "1                   0.05667  ...         24.99          23.41   \n",
       "2                   0.05999  ...         23.57          25.53   \n",
       "3                   0.09744  ...         14.91          26.50   \n",
       "4                   0.05883  ...         22.54          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "376                 0.08450  ...         10.85          22.82   \n",
       "377                 0.05763  ...         14.69          35.63   \n",
       "378                 0.05897  ...         14.54          19.64   \n",
       "379                 0.07950  ...         13.24          32.82   \n",
       "380                 0.06914  ...         12.84          20.53   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0            0.1622             0.6656   \n",
       "1             158.80      1956.0            0.1238             0.1866   \n",
       "2             152.50      1709.0            0.1444             0.4245   \n",
       "3              98.87       567.7            0.2098             0.8663   \n",
       "4             152.20      1575.0            0.1374             0.2050   \n",
       "..               ...         ...               ...                ...   \n",
       "376            76.51       351.9            0.1143             0.3619   \n",
       "377            97.11       680.6            0.1108             0.1457   \n",
       "378            97.96       657.0            0.1275             0.3104   \n",
       "379            91.76       508.1            0.2184             0.9379   \n",
       "380            84.93       476.1            0.1610             0.2429   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0            0.71190               0.26540          0.4601   \n",
       "1            0.24160               0.18600          0.2750   \n",
       "2            0.45040               0.24300          0.3613   \n",
       "3            0.68690               0.25750          0.6638   \n",
       "4            0.40000               0.16250          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "376          0.60300               0.14650          0.2597   \n",
       "377          0.07934               0.05781          0.2694   \n",
       "378          0.25690               0.10540          0.3387   \n",
       "379          0.84020               0.25240          0.4154   \n",
       "380          0.22470               0.13180          0.3343   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "376                  0.12000  \n",
       "377                  0.07061  \n",
       "378                  0.09638  \n",
       "379                  0.14030  \n",
       "380                  0.09215  \n",
       "\n",
       "[381 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.33, shuffle= False)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05864b3d",
   "metadata": {},
   "source": [
    "### Making a hundred models manually to check their accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4c32b",
   "metadata": {},
   "source": [
    "### list comprehension پایتون راهی برای ایجاد یک لیست بر اساس لیستی دیگر است. list comprehension معمولاً برای فیلتر کردن موارد از لیست یا تغییر مقادیر موجود در لیست استفاده می شود. list comprehension در داخل پرانتز قرار می گیرد. هنگامی که با لیست ها کار می کنید، ممکن است بخواهید یک لیست بر اساس محتویات یک دنباله موجود ایجاد کنید. به عنوان مثال، ممکن است بخواهید یک لیست بر اساس دنباله ای از کاراکترها ایجاد کنید. یا ممکن است بخواهید لیستی را ایجاد کنید که محتویات یک لیست دیگر را در دو ضرب کند.\n",
    "\n",
    "### میخوام یک لیستی بسازم و از ۱ تا ۲۰ رو بریزم داخلش. برا اینکار میتونید از list comprehension استفاده کنید. به شکل زیر:\n",
    "\n",
    "### v = [i for i in range(1, 21)]\n",
    "\n",
    "### print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aecb8",
   "metadata": {},
   "source": [
    "### Here we can easily create hundreds of models using lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2c4c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(max_depth=2),\n",
       " DecisionTreeClassifier(max_depth=3),\n",
       " DecisionTreeClassifier(max_depth=4),\n",
       " DecisionTreeClassifier(max_depth=5),\n",
       " DecisionTreeClassifier(max_depth=6),\n",
       " DecisionTreeClassifier(max_depth=7),\n",
       " DecisionTreeClassifier(max_depth=8),\n",
       " DecisionTreeClassifier(max_depth=9),\n",
       " DecisionTreeClassifier(max_depth=10),\n",
       " DecisionTreeClassifier(max_depth=11),\n",
       " DecisionTreeClassifier(max_depth=12),\n",
       " DecisionTreeClassifier(max_depth=13),\n",
       " DecisionTreeClassifier(max_depth=14),\n",
       " DecisionTreeClassifier(max_depth=15),\n",
       " DecisionTreeClassifier(max_depth=16),\n",
       " DecisionTreeClassifier(max_depth=17),\n",
       " DecisionTreeClassifier(max_depth=18),\n",
       " DecisionTreeClassifier(max_depth=19),\n",
       " DecisionTreeClassifier(max_depth=20),\n",
       " DecisionTreeClassifier(max_depth=21),\n",
       " DecisionTreeClassifier(max_depth=22),\n",
       " DecisionTreeClassifier(max_depth=23),\n",
       " DecisionTreeClassifier(max_depth=24),\n",
       " DecisionTreeClassifier(max_depth=25),\n",
       " DecisionTreeClassifier(max_depth=26),\n",
       " DecisionTreeClassifier(max_depth=27),\n",
       " DecisionTreeClassifier(max_depth=28),\n",
       " DecisionTreeClassifier(max_depth=29),\n",
       " DecisionTreeClassifier(max_depth=30),\n",
       " DecisionTreeClassifier(max_depth=31),\n",
       " DecisionTreeClassifier(max_depth=32),\n",
       " DecisionTreeClassifier(max_depth=33),\n",
       " DecisionTreeClassifier(max_depth=34),\n",
       " DecisionTreeClassifier(max_depth=35),\n",
       " DecisionTreeClassifier(max_depth=36),\n",
       " DecisionTreeClassifier(max_depth=37),\n",
       " DecisionTreeClassifier(max_depth=38),\n",
       " DecisionTreeClassifier(max_depth=39),\n",
       " DecisionTreeClassifier(max_depth=40),\n",
       " DecisionTreeClassifier(max_depth=41),\n",
       " DecisionTreeClassifier(max_depth=42),\n",
       " DecisionTreeClassifier(max_depth=43),\n",
       " DecisionTreeClassifier(max_depth=44),\n",
       " DecisionTreeClassifier(max_depth=45),\n",
       " DecisionTreeClassifier(max_depth=46),\n",
       " DecisionTreeClassifier(max_depth=47),\n",
       " DecisionTreeClassifier(max_depth=48),\n",
       " DecisionTreeClassifier(max_depth=49),\n",
       " DecisionTreeClassifier(max_depth=50),\n",
       " DecisionTreeClassifier(max_depth=51),\n",
       " DecisionTreeClassifier(max_depth=52),\n",
       " DecisionTreeClassifier(max_depth=53),\n",
       " DecisionTreeClassifier(max_depth=54),\n",
       " DecisionTreeClassifier(max_depth=55),\n",
       " DecisionTreeClassifier(max_depth=56),\n",
       " DecisionTreeClassifier(max_depth=57),\n",
       " DecisionTreeClassifier(max_depth=58),\n",
       " DecisionTreeClassifier(max_depth=59),\n",
       " DecisionTreeClassifier(max_depth=60),\n",
       " DecisionTreeClassifier(max_depth=61),\n",
       " DecisionTreeClassifier(max_depth=62),\n",
       " DecisionTreeClassifier(max_depth=63),\n",
       " DecisionTreeClassifier(max_depth=64),\n",
       " DecisionTreeClassifier(max_depth=65),\n",
       " DecisionTreeClassifier(max_depth=66),\n",
       " DecisionTreeClassifier(max_depth=67),\n",
       " DecisionTreeClassifier(max_depth=68),\n",
       " DecisionTreeClassifier(max_depth=69),\n",
       " DecisionTreeClassifier(max_depth=70),\n",
       " DecisionTreeClassifier(max_depth=71),\n",
       " DecisionTreeClassifier(max_depth=72),\n",
       " DecisionTreeClassifier(max_depth=73),\n",
       " DecisionTreeClassifier(max_depth=74),\n",
       " DecisionTreeClassifier(max_depth=75),\n",
       " DecisionTreeClassifier(max_depth=76),\n",
       " DecisionTreeClassifier(max_depth=77),\n",
       " DecisionTreeClassifier(max_depth=78),\n",
       " DecisionTreeClassifier(max_depth=79),\n",
       " DecisionTreeClassifier(max_depth=80),\n",
       " DecisionTreeClassifier(max_depth=81),\n",
       " DecisionTreeClassifier(max_depth=82),\n",
       " DecisionTreeClassifier(max_depth=83),\n",
       " DecisionTreeClassifier(max_depth=84),\n",
       " DecisionTreeClassifier(max_depth=85),\n",
       " DecisionTreeClassifier(max_depth=86),\n",
       " DecisionTreeClassifier(max_depth=87),\n",
       " DecisionTreeClassifier(max_depth=88),\n",
       " DecisionTreeClassifier(max_depth=89),\n",
       " DecisionTreeClassifier(max_depth=90),\n",
       " DecisionTreeClassifier(max_depth=91),\n",
       " DecisionTreeClassifier(max_depth=92),\n",
       " DecisionTreeClassifier(max_depth=93),\n",
       " DecisionTreeClassifier(max_depth=94),\n",
       " DecisionTreeClassifier(max_depth=95),\n",
       " DecisionTreeClassifier(max_depth=96),\n",
       " DecisionTreeClassifier(max_depth=97),\n",
       " DecisionTreeClassifier(max_depth=98),\n",
       " DecisionTreeClassifier(max_depth=99),\n",
       " DecisionTreeClassifier(max_depth=100),\n",
       " DecisionTreeClassifier(max_depth=101)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Decision tree models\n",
    "# When each model is created, its depth increases until the number reaches 101\n",
    "decision_tree_models = [DecisionTreeClassifier(max_depth= i) for i in range(2,102)]\n",
    "decision_tree_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9651ef8",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14af81b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=101)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=101)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=101)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train each model\n",
    "for model in decision_tree_models:\n",
    "    model.fit(x_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01770b",
   "metadata": {},
   "source": [
    "### Check accuracy score for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85057f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:   0.8936170212765957\n",
      "Accuracy score:   0.8936170212765957\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.9095744680851063\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.851063829787234\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8563829787234043\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.9148936170212766\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.9148936170212766\n",
      "Accuracy score:   0.8936170212765957\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.851063829787234\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8829787234042553\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8829787234042553\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8563829787234043\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.898936170212766\n",
      "Accuracy score:   0.8829787234042553\n",
      "Accuracy score:   0.9095744680851063\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.851063829787234\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8829787234042553\n",
      "Accuracy score:   0.8457446808510638\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8829787234042553\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8563829787234043\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8457446808510638\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.8829787234042553\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8882978723404256\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8563829787234043\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.9042553191489362\n",
      "Accuracy score:   0.851063829787234\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.851063829787234\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8457446808510638\n",
      "Accuracy score:   0.8776595744680851\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8936170212765957\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8723404255319149\n",
      "Accuracy score:   0.8617021276595744\n",
      "Accuracy score:   0.8670212765957447\n",
      "Accuracy score:   0.8457446808510638\n",
      "Accuracy score:   0.8670212765957447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in decision_tree_models:\n",
    "    y_pre = model.predict(x_test)\n",
    "    accur = accuracy_score(y_pre, y_test)\n",
    "    print('Accuracy score:  ' , accur)\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816223c",
   "metadata": {},
   "source": [
    "### Modeling with voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e64e48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:   0.8829787234042553\n"
     ]
    }
   ],
   "source": [
    "# As input to the method we need to send a list of tuples (estimators)\n",
    "# enumerate can return us index and item\n",
    "voting = VotingClassifier(estimators=[(f'model_{i}', model) for i, model in enumerate(decision_tree_models)])\n",
    "voting.fit(x_train, y_train)\n",
    "voting_pre = voting.predict(x_test)\n",
    "voting_accur = accuracy_score(voting_pre, y_test)\n",
    "print('Accuracy score:  ' , voting_accur)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
